{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력된 수를 1, 11, 21, 31 ...만들어 주는 함수\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        page_num1 = num\n",
    "        return page_num1\n",
    "    elif num == 0:\n",
    "        page_num2 = num+1\n",
    "        return page_num2\n",
    "    else:\n",
    "        page_num3 = num+9*(num-1)\n",
    "        return page_num3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search,start_pg,end_pg,start_date, end_date):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + f'&sm=tab_opt&nso=so:r,p:from{start_date}to{end_date}'+\"&start=\" + str(start_page)\n",
    "        print(\"생성url: \",url)\n",
    "        return url\n",
    "    else:\n",
    "        urls= []\n",
    "        for i in range(start_pg, end_pg+1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + f'&sm=tab_opt&nso=so:r,p:from{start_date}to{end_date}' + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성url: \",urls)\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links (search_urls) :\n",
    "  headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "  naver_urls = []\n",
    "  press_name = []\n",
    "\n",
    "\n",
    "  cnt = 0\n",
    "  for i in search_urls :\n",
    "      cnt =+ 1\n",
    "      print(\"-----------> \", cnt)\n",
    "      print(\"search_urls: \", i)\n",
    "      response = requests.get(i, headers=headers)\n",
    "      html = response.text\n",
    "      soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "      news = soup.select(\"div > div.news_info > div.info_group > a \")     # a:nth-of-type(3)\n",
    "      article = soup.select(\"a.info.press\") # i당 10개\n",
    "      articles = []\n",
    "      for art in article:\n",
    "        articles.append(art)\n",
    "\n",
    "      #print(\"\\n뉴스 : \",news)\n",
    "      #print(\"\\n아티클 : \", article)\n",
    "      cnt = 0\n",
    "      idx = 0\n",
    "      \n",
    "      for n in news:\n",
    "        cnt += 1\n",
    "        print(\"-- \", cnt, \" : \", n)\n",
    "\n",
    "        if \"sid1=106\" in n['href'] :\n",
    "              pass\n",
    "        elif \"sports\" in n['href'] :\n",
    "              pass\n",
    "            \n",
    "        elif  n['href'].startswith(\"https://news.naver.com\"):\n",
    "              print(\"\\n네이버뉴스 입니다\")\n",
    "              news_url = n['href']\n",
    "              naver_urls.append(news_url)\n",
    "              try:\n",
    "                press_name.append(articles[idx-1].text.replace(\"선정\",\"\"))\n",
    "              except:\n",
    "                print('none')\n",
    "              \n",
    "              \n",
    "        else:\n",
    "          idx += 1\n",
    "          \n",
    "  return naver_urls, press_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content (naver_urls) : \n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "\n",
    "    titles = []\n",
    "    contents = []\n",
    "    news_date = []\n",
    "    report = []\n",
    "\n",
    "    for i in naver_urls :\n",
    "        original_html = requests.get (i, headers=headers)\n",
    "        h = original_html.content\n",
    "        html = BeautifulSoup(h, \"html.parser\")\n",
    "        title = html.select(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        title = ''.join(str(title)) # 문자열 합치기\n",
    "        \n",
    "        del_str = '<[^>]*>'\n",
    "        title = re.sub(pattern=del_str, repl='',string=title)\n",
    "        titles.append(title) \n",
    "        \n",
    "        # 본문\n",
    "        content = html.select(\"#dic_area\")\n",
    "        content = ''.join(str(content))\n",
    "        content = re.sub(pattern=del_str, repl='',string=content)\n",
    "        del_str2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(del_str2,'')\n",
    "        # print(contents)\n",
    "        contents.append(content)\n",
    "        \n",
    "        # 기사작성일\n",
    "        p_date = html.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")\n",
    "        4\n",
    "        \n",
    "        if  \"sid1=106\" in i:\n",
    "            sports = html.select(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "            pattern1=r'([0-9]{4})-([0-9]{2})-([0-9]{2})'\n",
    "            regex_result1 = re.search(pattern1, str(sports))\n",
    "            pub_date1 = regex_result1.group(0)\n",
    "            news_date.append(pub_date1)\n",
    "                    \n",
    "        else : \n",
    "            pattern=r'([0-9]{4})-([0-9]{2})-([0-9]{2})'\n",
    "            regex_result = re.search(pattern, str(p_date))\n",
    "            pub_date = regex_result.group(0)\n",
    "            news_date.append(pub_date)\n",
    "    \n",
    "        # 기자이름\n",
    "        \n",
    "        r = html.select(\"#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_journalist > a > em\")\n",
    "        if len(r) == 0:\n",
    "            r = html.select(\"#contents > div.byline > p > span\")\n",
    "        report.append(r)\n",
    "    \n",
    "    reporters = []\n",
    "\n",
    "    for a in report :\n",
    "        if len(a) ==0 :\n",
    "            reporters.append(r)\n",
    "        \n",
    "        else :\n",
    "            b = a[0].text # 위치값 \n",
    "            reporters.append(b.replace(' 기자',''))\n",
    "\n",
    "    return titles, contents, news_date, reporters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFrame (news_date, press_name, reporters, naver_urls, titles, contents):\n",
    "    data = pd.DataFrame({\"Pub_date\" : news_date, \"Press_name\" : press_name, \"Reporter\" : reporters, \"News_url\" : naver_urls, \"Title\" : titles, \"Content\" : contents})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검색어 입력\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "\n",
    "#검색시작일\n",
    "start_date = input(\"검색 시작 날짜 (YYYYmmdd) : \")\n",
    "\n",
    "#검색종료일\n",
    "end_date = input(\"검색 종료 날짜 (YYYYmmdd) : \")\n",
    "\n",
    "#검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\"))\n",
    "print(\"\\n크롤링할 시작 페이지: \",page,\"페이지\")   \n",
    "#검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\"))\n",
    "print(\"\\n크롤링할 종료 페이지: \",page2,\"페이지\")   \n",
    "\n",
    "# naver url 생성\n",
    "search_urls = makeUrl(search,page, page2,start_date, end_date)\n",
    "naver_urls, press_name = get_links (search_urls)\n",
    "titles, contents, news_date, reporters = get_content (naver_urls)\n",
    "data = makeFrame (news_date, press_name, reporters, naver_urls, titles, contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Title\"] = data[\"Title\"].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)\n",
    "data[\"Title\"] = data[\"Title\"].str.replace(\"[0-9]+\", repl=r\" \", regex=True)\n",
    "data[\"Title\"] = data[\"Title\"].str.replace(pat=r'[^ㄱ-ㅎㅏ-ㅣ가-힣]',repl=r\" \",regex=True)\n",
    "\n",
    "data[\"Content\"] = data[\"Content\"].str.replace(pat=r'(\\[a-zA-Z0-9\\_.+-\\]+@\\[a-zA-Z0-9-\\]+.\\[a-zA-Z0-9-.\\]+)', repl=r'', regex=True)\n",
    "data[\"Content\"] = data[\"Content\"].str.replace(pat=r'[^ㄱ-ㅎㅏ-ㅣ가-힣]',repl=r\" \",regex=True)\n",
    "data[\"Content\"] = data[\"Content\"].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)\n",
    "data[\"Content\"] = data[\"Content\"].str.replace(pat=r'[\\r|\\n]', repl=r' ', regex=True)\n",
    "data[\"Content\"] = data[\"Content\"].str.replace(pat=r'<[^>]*>', repl=r' ', regex=True)\n",
    "data[\"Content\"] = data[\"Content\"].str.replace(\"[0-9]+\", repl=r\" \", regex=True)\n",
    "\n",
    "data['Reporter'] = data['Reporter'].str.replace(pat=r'[^ㄱ-ㅎㅏ-ㅣ가-힣]',repl=r\" \",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\"기타\")\n",
    "\n",
    "# 중복 확인 후 제거\n",
    "final_df = data.copy()\n",
    "dup = final_df.duplicated(['News_url']).sum()\n",
    "print(dup)\n",
    "final_df=final_df.drop_duplicates(['News_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 저장\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = \"{}\".format(search) + \"_\" + \"{}\".format(end_date)\n",
    "df = final_df.to_csv(filename +'.csv', encoding = \"utf-8-sig\", index = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
