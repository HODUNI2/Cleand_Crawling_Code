{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제목, 링크, 발행일자 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵션 사항이 반영된 블로그 링크 리스트\n",
    "\n",
    "def nv_crawling(keyword): \n",
    "    # 웹 드라이버 불러오기\n",
    "    browser = webdriver.Chrome(executable_path=r'C:\\Users\\User\\Desktop\\drivers\\chromedriver.exe', options=options)\n",
    "    browser.implicitly_wait(2)\n",
    "    \n",
    "    # 날짜 리스트\n",
    "    start_date = [20210101, 20210201, 20210301, 20210401, 20210501, 20210601, 20210701, 20210801, 20210901, 20211001, 20211101, 20211201]\n",
    "    end_date = [20210131, 20210228, 20210331, 20210430, 20210531, 20210630, 20210731, 20210831, 20210930, 20211031, 20211130, 20211231]\n",
    "    # 블로그 제목, 링크, 발행날짜 뽑기\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "    pubdate_list = []\n",
    "    \n",
    "    for st, en in zip(start_date, end_date):\n",
    "        # 사이트 주소는 네이버\n",
    "        url = 'https://search.naver.com/search.naver?where=blog&query=' + quote(keyword) + f'&sm=tab_opt&nso=so:r,p:from{st}to{en}'\n",
    "        browser.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 무한 스크롤 내리기(숫자만큼 내려감)\n",
    "        for c in range(1, 500): \n",
    "            browser.find_element(By.TAG_NAME,'body').send_keys(Keys.PAGE_DOWN)\n",
    "            browser.implicitly_wait(10)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        # 블로그 포스팅 제목, 주소가 담긴 몸통\n",
    "        items= browser.find_elements_by_class_name('total_area')\n",
    "        #len(items)\n",
    "        for item in items:\n",
    "            print('\\n') # 문자열 줄바꿈\n",
    "            \n",
    "            # 블로그 게시글 제목\n",
    "            title = item.find_element_by_class_name('total_tit').text\n",
    "            title_list.append(title)\n",
    "            print('제목 : ', title)      \n",
    "            \n",
    "            # 블로그 링크 : 브런치, 다음, 티스토리 공백\n",
    "            link = item.find_element_by_class_name('total_tit').get_attribute('href')\n",
    "            \n",
    "            if 'naver' in link:\n",
    "                link_list.append(link)\n",
    "\n",
    "            elif 'brunch' in link: # brunch 블로그가 나오면 공백으로 남기기\n",
    "                link_list.append(np.nan)\n",
    "\n",
    "            elif 'daum' in link: # brunch 블로그가 나오면 공백으로 남기기\n",
    "                link_list.append(np.nan)\n",
    "\n",
    "            elif 'tistory' in link: # brunch 블로그가 나오면 공백으로 남기기\n",
    "                link_list.append(np.nan)\n",
    "            \n",
    "            else: \n",
    "                link_list.append(np.nan)\n",
    "\n",
    "            # 발행일자\n",
    "            pubdate = item.find_element_by_class_name('sub_time').text\n",
    "            pubdate = pubdate.rstrip('.') # 오른쪽 끝단의 '.' 삭제\n",
    "            pubdate = pubdate.replace('.', '-')\n",
    "\n",
    "            # 3시간 전, 4일 전, 어제 따로 처리\n",
    "            now = datetime.datetime.now() # 오늘 날짜 불러오기\n",
    "            if '시간 전' in pubdate:\n",
    "                pubdate = now.strftime('%Y-%m-%d')\n",
    "                pubdate_list.append(pubdate)\n",
    "            elif '일 전' in pubdate:\n",
    "                temp = int(pubdate[0:1])\n",
    "                temp = datetime.timedelta(days=temp)\n",
    "                pubdate = datetime.datetime.now() - temp\n",
    "                pubdate = pubdate.strftime('%Y-%m-%d')\n",
    "                pubdate_list.append(pubdate)\n",
    "            elif '어제' in pubdate:\n",
    "                temp = datetime.timedelta(days=1)\n",
    "                pubdate = datetime.datetime.now() - temp\n",
    "                pubdate = pubdate.strftime('%Y-%m-%d')\n",
    "                pubdate_list.append(pubdate)\n",
    "            else:\n",
    "                pubdate_list.append(pubdate)\n",
    "            print('발행일자 : ',pubdate)\n",
    "    print('크롤링한 총 개수 :', len(title_list))    \n",
    "    return title_list, link_list, pubdate_list \n",
    "    #브라우저 닫기\n",
    "    browser.quit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본문 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contents_crawling(link_list):\n",
    "    browser = webdriver.Chrome(executable_path=r'C:\\Users\\User\\Desktop\\drivers\\chromedriver.exe', options=options)\n",
    "    browser.implicitly_wait(2)\n",
    "    \n",
    "    # 위에서 생성한 css selector list 하나씩 클릭하여 본문 url얻기\n",
    "    content_list = []\n",
    "    for i in link_list:\n",
    "        try:\n",
    "            # 블로그 링크 하나씩 불러오기\n",
    "            browser.get(i)\n",
    "            time.sleep(1)\n",
    "            # 블로그 안 본문이 있는 iframe에 접근하기\n",
    "            browser.switch_to.frame(\"mainFrame\")\n",
    "            # 본문 내용 크롤링하기\n",
    "            try:\n",
    "                a = browser.find_element_by_css_selector('div.se-main-container').text\n",
    "                content_list.append(a)\n",
    "            except:  # NoSuchElement 오류시 예외처리(구버전 블로그에 적용)\n",
    "                a = browser.find_element_by_css_selector('div#content-area').text\n",
    "                content_list.append(a)\n",
    "            # print(본문: \\n', a)\n",
    "        except: # 네블 외 블로그들은 공백으로 넘어가기\n",
    "            content_list.append('')\n",
    "            continue # 공백은 무시한 채 계속 진행\n",
    "    print('<<본문 크롤링이 완료되었습니다.>>')\n",
    "    return content_list\n",
    "   \n",
    "    # 브라우저 닫기\n",
    "    browser.quit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제목, 링크, 발행일자를 Dataframe으로 만들기 : 엑셀로 저장\n",
    "def df_make(title_list, link_list, pubdate_list, content_list):\n",
    "    df = pd.DataFrame({'제목':title_list, '링크':link_list, '발행일자':pubdate_list, '본문':content_list})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 함수 : 함수 한꺼번에 실행\n",
    "if __name__=='__main__':\n",
    "    # 날짜 받기\n",
    "    keyword = input('키워드를 입력해주세요 : ')\n",
    "    title_list, link_list, pubdate_list = nv_crawling(keyword)\n",
    "    content_list = contents_crawling(link_list)\n",
    "    df = df_make(title_list, link_list, pubdate_list, content_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DB에 DF넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "conn_str = 'host=localhost dbname=naver_blog user=postgres password=1234 port=5432'\n",
    "try: \n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    print('=====접속 성공=====')\n",
    "except psycopg2.DatabaseError as db_err:\n",
    "    print('접속오류!!')\n",
    "    print(db_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 커서 생성\n",
    "cur = conn.cursor()\n",
    "\n",
    "# engine 생성\n",
    "engine = create_engine('postgresql://postgres:1234@localhost:5432/naver_blog')\n",
    "\n",
    "# 실행할 때마다 다른 값이 나오지 않게 테이블 제거\n",
    "cur.execute('DROP TABLE IF EXISTS 캠핑2021')\n",
    "\n",
    "df.to_sql(name='캠핑2021',\n",
    "        con = engine,\n",
    "        schema='public',\n",
    "        if_exists='replace', # {'fail', 'replace', 'append'}, dafault : 'fail'\n",
    "        index= True,\n",
    "        index_label='id',\n",
    "        chunksize= 100,\n",
    "        dtype= {\n",
    "            'id' : sqlalchemy.types.INTEGER(),\n",
    "            '제목' : sqlalchemy.types.VARCHAR(200),\n",
    "            '링크' : sqlalchemy.types.VARCHAR(200),\n",
    "            '발행일자' : sqlalchemy.types.VARCHAR(20),\n",
    "            '본문' : sqlalchemy.types.VARCHAR(200000)\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
